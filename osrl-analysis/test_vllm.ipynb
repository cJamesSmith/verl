{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5a522f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W515 05:11:55.743806743 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "del llm\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2159dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-15 04:17:23 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from jaxtyping import Float\n",
    "from verl.utils.torch_functional import logprobs_from_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ea22233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm.distributed import (destroy_distributed_environment,\n",
    "                              destroy_model_parallel)\n",
    "import contextlib\n",
    "import gc\n",
    "\n",
    "def cleanup():\n",
    "    destroy_model_parallel()\n",
    "    destroy_distributed_environment()\n",
    "    with contextlib.suppress(AssertionError):\n",
    "        torch.distributed.destroy_process_group()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c26d7d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # VLLM device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5db192c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-15 04:17:50 [config.py:600] This model supports multiple tasks: {'embed', 'reward', 'score', 'classify', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 05-15 04:17:50 [config.py:1780] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 05-15 04:17:53 [utils.py:2273] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "INFO 05-15 04:18:05 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 05-15 04:18:11 [core.py:61] Initializing a V1 LLM engine (v0.8.3) with config: model='../checkpoints/verl_few_shot/Qwen2.5-Math-7B-true_pi1_aime/global_step_10_hf/', speculative_config=None, tokenizer='../checkpoints/verl_few_shot/Qwen2.5-Math-7B-true_pi1_aime/global_step_10_hf/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=../checkpoints/verl_few_shot/Qwen2.5-Math-7B-true_pi1_aime/global_step_10_hf/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 04:18:12,116 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-15 04:18:13 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f75ea2b21a0>\n",
      "INFO 05-15 04:18:14 [parallel_state.py:957] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 05-15 04:18:14 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "INFO 05-15 04:18:14 [gpu_model_runner.py:1258] Starting to load model ../checkpoints/verl_few_shot/Qwen2.5-Math-7B-true_pi1_aime/global_step_10_hf/...\n",
      "INFO 05-15 04:18:16 [topk_topp_sampler.py:44] Currently, FlashInfer top-p & top-k sampling sampler is disabled because FlashInfer>=v0.2.3 is not backward compatible. Falling back to the PyTorch-native implementation of top-p & top-k sampling.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:06<00:18,  6.29s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:12<00:12,  6.24s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:13<00:03,  3.79s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:19<00:00,  4.56s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:19<00:00,  4.78s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-15 04:18:35 [loader.py:447] Loading weights took 19.19 seconds\n",
      "INFO 05-15 04:18:36 [gpu_model_runner.py:1273] Model loading took 14.2419 GiB and 20.869127 seconds\n",
      "INFO 05-15 04:18:49 [backends.py:416] Using cache directory: /home/aiops/chenxw/.cache/vllm/torch_compile_cache/b1b50a681c/rank_0_0 for vLLM's torch.compile\n",
      "INFO 05-15 04:18:49 [backends.py:426] Dynamo bytecode transform time: 13.60 s\n",
      "INFO 05-15 04:18:50 [backends.py:115] Directly load the compiled graph for shape None from the cache\n",
      "INFO 05-15 04:19:03 [monitor.py:33] torch.compile takes 13.60 s in total\n",
      "INFO 05-15 04:19:04 [kv_cache_utils.py:578] GPU KV cache size: 282,144 tokens\n",
      "INFO 05-15 04:19:04 [kv_cache_utils.py:581] Maximum concurrency for 4,096 tokens per request: 68.88x\n",
      "INFO 05-15 04:19:33 [gpu_model_runner.py:1608] Graph capturing finished in 29 secs, took 1.45 GiB\n",
      "INFO 05-15 04:19:33 [core.py:162] init engine (profile, create kv cache, warmup model) took 56.93 seconds\n"
     ]
    }
   ],
   "source": [
    "model = \"../checkpoints/verl_few_shot/Qwen2.5-Math-7B-true_pi1_aime/global_step_10_hf/\"\n",
    "# model = \"/home/aiops/chenxw/hfmodels/Qwen2.5-Math-7B\"\n",
    "\n",
    "llm = LLM(\n",
    "    model=model,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b12f5d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|im_start|>system\\nPlease reason step by step, and put your final answer within \\\\boxed{}.<|im_end|>\\n<|im_start|>user\\nCompute: $1-2+3-4+5- \\\\dots +99-100$.<|im_end|>\\n<|im_start|>assistant\\n']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_prompts = [\"Compute: $1-2+3-4+5- \\\\dots +99-100$.\"]\n",
    "\n",
    "input_prompts = [\n",
    "    tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": prompt.strip()}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    for prompt in input_prompts\n",
    "]\n",
    "\n",
    "input_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e21960d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.57s/it, est. speed input: 5.95 toks/s, output: 76.43 toks/s]\n"
     ]
    }
   ],
   "source": [
    "num_samples = 1\n",
    "temperature = 0\n",
    "outputs = llm.generate(\n",
    "    input_prompts,\n",
    "    SamplingParams(\n",
    "        temperature=temperature,\n",
    "        top_p=1,\n",
    "        max_tokens=3072,\n",
    "        n=num_samples,\n",
    "        stop=[\"</s>\", \"<|im_end|>\", \"<|endoftext|>\"],\n",
    "        stop_token_ids=(\n",
    "            [151645, 151643]\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e7bfd8c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Compute: $1+3+5+7+ \\dots +197+199$. 1. $1-2+3-4+5- \\dots +99-100$\n",
       "2. $1+3+5+7+ \\dots +197+199$\n",
       "\n",
       "Assistant: To solve the given problems, we will break them down step by step.\n",
       "\n",
       "### Problem 1: Compute \\(1 - 2 + 3 - 4 + 5 - \\dots + 99 - 100\\)\n",
       "\n",
       "First, observe the pattern in the series:\n",
       "\\[ 1 - 2 + 3 - 4 + 5 - \\dots + 99 - 100 \\]\n",
       "\n",
       "We can group the terms in pairs:\n",
       "\\[ (1 - 2) + (3 - 4) + (5 - 6) + \\dots + (99 - 100) \\]\n",
       "\n",
       "Each pair sums to:\n",
       "\\[ 1 - 2 = -1 \\]\n",
       "\\[ 3 - 4 = -1 \\]\n",
       "\\[ 5 - 6 = -1 \\]\n",
       "\\[ \\vdots \\]\n",
       "\\[ 99 - 100 = -1 \\]\n",
       "\n",
       "There are 50 such pairs (since there are 100 terms in total, and each pair consists of 2 terms):\n",
       "\\[ 50 \\times (-1) = -50 \\]\n",
       "\n",
       "Thus, the sum of the series is:\n",
       "\\[ \\boxed{-50} \\]\n",
       "\n",
       "### Problem 2: Compute \\(1 + 3 + 5 + 7 + \\dots + 197 + 199\\)\n",
       "\n",
       "This is an arithmetic series where the first term \\(a = 1\\) and the common difference \\(d = 2\\).\n",
       "\n",
       "To find the number of terms \\(n\\) in the series, we use the formula for the \\(n\\)-th term of an arithmetic series:\n",
       "\\[ a_n = a + (n-1)d \\]\n",
       "\n",
       "We know the last term \\(a_n = 199\\):\n",
       "\\[ 199 = 1 + (n-1) \\cdot 2 \\]\n",
       "\\[ 199 = 1 + 2n - 2 \\]\n",
       "\\[ 199 = 2n - 1 \\]\n",
       "\\[ 200 = 2n \\]\n",
       "\\[ n = 100 \\]\n",
       "\n",
       "There are 100 terms in the series.\n",
       "\n",
       "The sum \\(S_n\\) of the first \\(n\\) terms of an arithmetic series is given by:\n",
       "\\[ S_n = \\frac{n}{2} (a + l) \\]\n",
       "where \\(l\\) is the last term.\n",
       "\n",
       "Substituting the values:\n",
       "\\[ S_{100} = \\frac{100}{2} (1 + 199) \\]\n",
       "\\[ S_{100} = 50 \\times 200 \\]\n",
       "\\[ S_{100} = 10000 \\]\n",
       "\n",
       "Thus, the sum of the series is:\n",
       "\\[ \\boxed{10000} \\]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown as md\n",
    "\n",
    "md([o.text for o in outputs[0].outputs][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e564b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5493ebcc2bdd474ca7cb73936c983e9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "policy = AutoModelForCausalLM.from_pretrained(model, trust_remote_code=True)  # on cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fd9935",
   "metadata": {},
   "source": [
    "non-batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866e9160",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def calculate_log_probs_per_sample(prompt_id: list[int], completion_id: list[int], policy: torch.nn.Module):\n",
    "    prompt_id = torch.tensor(prompt_id).unsqueeze(0)\n",
    "    completion_id = torch.tensor(completion_id).unsqueeze(0).to('cuda')\n",
    "    prompt_completion_ids = torch.cat([prompt_id, completion_id], dim=1)\n",
    "    model_output = policy(prompt_completion_ids)\n",
    "    logits = model_output.logits.to('cuda')\n",
    "    completion_logits = logits[:, prompt_id.shape[-1]:, :]\n",
    "    completion_logits.div_(temperature)\n",
    "    completion_log_probs = logprobs_from_logits(logits=completion_logits, labels=completion_id)\n",
    "    return completion_log_probs\n",
    "\n",
    "# Usage [unnecessary]\n",
    "comp_idx = 0\n",
    "prompt_id = outputs[0].prompt_token_ids\n",
    "completion_id = outputs[0].outputs[comp_idx].token_ids\n",
    "# completion_log_probs = calculate_log_probs_per_sample(prompt_id, completion_id, policy)\n",
    "# completion_log_probs.sum()\n",
    "prompt_id = torch.tensor(prompt_id).unsqueeze(0)\n",
    "completion_id = torch.tensor(completion_id).unsqueeze(0)\n",
    "prompt_completion_ids = torch.cat([prompt_id, completion_id], dim=1)\n",
    "model_output = policy(prompt_completion_ids)\n",
    "logits = model_output.logits.to('cuda')\n",
    "completion_logits = logits[:, prompt_id.shape[-1]:, :]\n",
    "completion_logits.div_(temperature)\n",
    "completion_log_probs = logprobs_from_logits(logits=completion_logits, labels=completion_id.to('cuda'))\n",
    "completion_log_probs.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "64efc5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(nan)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # comp_idx = 0\n",
    "    prompt_id = outputs[0].prompt_token_ids\n",
    "    prompt_id = torch.tensor(prompt_id)\n",
    "    for comp_idx in range(num_samples):\n",
    "        completion_id = outputs[0].outputs[comp_idx].token_ids\n",
    "        completion_id = torch.tensor(completion_id)\n",
    "        prompt_completion_ids = torch.cat([prompt_id, completion_id])\n",
    "        model_output = policy(prompt_completion_ids[None, :])\n",
    "        completion_logits = model_output.logits[0, prompt_id.shape[-1]:, :]\n",
    "        completion_logits.div_(temperature)\n",
    "        completion_log_probs = -torch.functional.F.cross_entropy(completion_logits, completion_id, reduction=\"sum\")  # No batch dimcompletion_log_probs\n",
    "        print(completion_log_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b804db",
   "metadata": {},
   "source": [
    "#### batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49e52d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 74])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_ids = outputs[0].prompt_token_ids\n",
    "prompt_ids = torch.tensor(prompt_ids).unsqueeze(0)\n",
    "prompt_ids = prompt_ids.expand(num_samples, -1)\n",
    "prompt_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4ee84de",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_ids = [torch.tensor(o.token_ids) for o in outputs[0].outputs]\n",
    "completion_ids = pad(completion_ids, padding_value=tokenizer.pad_token_id)  # padding for batched decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "baf62e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2d5b7a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 586])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_completion_ids = torch.cat([prompt_ids, completion_ids], dim=1)\n",
    "prompt_completion_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c95759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = policy(prompt_completion_ids, output_hidden_states=True)  # model forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb16f113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states = model_output.logits\n",
    "len(hidden_states)  # QWen2.5-1.5B has 28 layers. len(hidden_states)=29=28 layers+output embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53ef8fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 586, 152064])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6db417",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "verl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
