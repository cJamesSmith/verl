{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03a62607",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1ea279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = \"/home/aiops/chenxw/hfmodels/Qwen3-4B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54b19529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-10 13:50:56 [config.py:717] This model supports multiple tasks: {'generate', 'embed', 'classify', 'score', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 07-10 13:50:56 [config.py:1770] Defaulting to use mp for distributed inference\n",
      "INFO 07-10 13:50:56 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 07-10 13:50:59 [core.py:58] Initializing a V1 LLM engine (v0.8.5) with config: model='/home/aiops/chenxw/hfmodels/Qwen3-4B', speculative_config=None, tokenizer='/home/aiops/chenxw/hfmodels/Qwen3-4B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=20000, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/aiops/chenxw/hfmodels/Qwen3-4B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 07-10 13:50:59 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 96 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 07-10 13:50:59 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 10485760, 10, 'psm_bdc22ed7'), local_subscribe_addr='ipc:///tmp/030a205a-232d-40e0-80c4-da15001e0096', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 13:51:00,349 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
      "2025-07-10 13:51:00,371 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
      "2025-07-10 13:51:00,435 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n",
      "2025-07-10 13:51:00,640 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-10 13:51:02 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f22a9b6d180>\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1229)\u001b[0;0m INFO 07-10 13:51:02 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_9d7a05a8'), local_subscribe_addr='ipc:///tmp/30a64b75-2ef0-46fa-9364-c6df233fb025', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 07-10 13:51:02 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f22a9b6d000>\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1228)\u001b[0;0m INFO 07-10 13:51:02 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_13109a5d'), local_subscribe_addr='ipc:///tmp/bbab7321-8c7a-42f6-b396-87595df32860', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 07-10 13:51:02 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f22a9b6ce80>\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1227)\u001b[0;0m INFO 07-10 13:51:02 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b7e8d220'), local_subscribe_addr='ipc:///tmp/8dd124fa-5b7b-4a40-bf07-549d40bfdc4c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 07-10 13:51:02 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f22a9b6ccd0>\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1226)\u001b[0;0m INFO 07-10 13:51:02 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_bc3e1ba4'), local_subscribe_addr='ipc:///tmp/36badec1-a0e5-4bcf-97c2-c83fe4a5654f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1229)\u001b[0;0m INFO 07-10 13:51:03 [utils.py:1055] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1229)\u001b[0;0m INFO 07-10 13:51:03 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1226)\u001b[0;0m INFO 07-10 13:51:03 [utils.py:1055] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1226)\u001b[0;0m INFO 07-10 13:51:03 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1228)\u001b[0;0m INFO 07-10 13:51:04 [utils.py:1055] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1228)\u001b[0;0m INFO 07-10 13:51:04 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1227)\u001b[0;0m INFO 07-10 13:51:04 [utils.py:1055] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1227)\u001b[0;0m INFO 07-10 13:51:04 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1226)\u001b[0;0m INFO 07-10 13:51:05 [custom_all_reduce_utils.py:206] generating GPU P2P access cache in /home/aiops/chenxw/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1226)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=1228)\u001b[0;0m INFO 07-10 13:51:57 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/aiops/chenxw/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1229)\u001b[0;0m INFO 07-10 13:51:57 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/aiops/chenxw/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 07-10 13:51:57 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/aiops/chenxw/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1227)\u001b[0;0m INFO 07-10 13:51:57 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/aiops/chenxw/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1226)\u001b[0;0m INFO 07-10 13:51:57 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_c1848a0a'), local_subscribe_addr='ipc:///tmp/3abf9d48-164b-4d52-a704-529a54ae474f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1229)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=1228)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=1226)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=1227)\u001b[0;0m INFO 07-10 13:51:57 [parallel_state.py:1004] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3\n",
      "INFO 07-10 13:51:57 [parallel_state.py:1004] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2\n",
      "INFO 07-10 13:51:57 [parallel_state.py:1004] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 07-10 13:51:57 [parallel_state.py:1004] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1226)\u001b[0;0m INFO 07-10 13:51:57 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1226)\u001b[0;0m INFO 07-10 13:51:57 [topk_topp_sampler.py:44] Currently, FlashInfer top-p & top-k sampling sampler is disabled because FlashInfer>=v0.2.3 is not backward compatible. Falling back to the PyTorch-native implementation of top-p & top-k sampling.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1226)\u001b[0;0m INFO 07-10 13:51:57 [gpu_model_runner.py:1329] Starting to load model /home/aiops/chenxw/hfmodels/Qwen3-4B...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1228)\u001b[0;0m INFO 07-10 13:51:57 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1228)\u001b[0;0m INFO 07-10 13:51:57 [topk_topp_sampler.py:44] Currently, FlashInfer top-p & top-k sampling sampler is disabled because FlashInfer>=v0.2.3 is not backward compatible. Falling back to the PyTorch-native implementation of top-p & top-k sampling.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1228)\u001b[0;0m INFO 07-10 13:51:57 [gpu_model_runner.py:1329] Starting to load model /home/aiops/chenxw/hfmodels/Qwen3-4B...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1229)\u001b[0;0m INFO 07-10 13:51:58 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1229)\u001b[0;0m INFO 07-10 13:51:58 [topk_topp_sampler.py:44] Currently, FlashInfer top-p & top-k sampling sampler is disabled because FlashInfer>=v0.2.3 is not backward compatible. Falling back to the PyTorch-native implementation of top-p & top-k sampling.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1227)\u001b[0;0m INFO 07-10 13:51:58 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1227)\u001b[0;0m INFO 07-10 13:51:58 [topk_topp_sampler.py:44] Currently, FlashInfer top-p & top-k sampling sampler is disabled because FlashInfer>=v0.2.3 is not backward compatible. Falling back to the PyTorch-native implementation of top-p & top-k sampling.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1229)\u001b[0;0m INFO 07-10 13:51:58 [gpu_model_runner.py:1329] Starting to load model /home/aiops/chenxw/hfmodels/Qwen3-4B...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1227)\u001b[0;0m INFO 07-10 13:51:58 [gpu_model_runner.py:1329] Starting to load model /home/aiops/chenxw/hfmodels/Qwen3-4B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c92b9e7aebe43ddad4874addf2e5c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=2 pid=1228)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=1227)\u001b[0;0m INFO 07-10 13:52:07 [loader.py:458] Loading weights took 9.40 seconds\n",
      "INFO 07-10 13:52:07 [loader.py:458] Loading weights took 9.44 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1229)\u001b[0;0m INFO 07-10 13:52:08 [loader.py:458] Loading weights took 9.44 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1226)\u001b[0;0m INFO 07-10 13:52:08 [loader.py:458] Loading weights took 9.54 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1228)\u001b[0;0m INFO 07-10 13:52:08 [gpu_model_runner.py:1347] Model loading took 1.8947 GiB and 10.109819 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1227)\u001b[0;0m INFO 07-10 13:52:08 [gpu_model_runner.py:1347] Model loading took 1.8947 GiB and 9.901504 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1229)\u001b[0;0m INFO 07-10 13:52:08 [gpu_model_runner.py:1347] Model loading took 1.8947 GiB and 9.987425 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1226)\u001b[0;0m INFO 07-10 13:52:08 [gpu_model_runner.py:1347] Model loading took 1.8947 GiB and 10.228815 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1228)\u001b[0;0m INFO 07-10 13:52:26 [backends.py:420] Using cache directory: /home/aiops/chenxw/.cache/vllm/torch_compile_cache/091db74a4d/rank_2_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1228)\u001b[0;0m INFO 07-10 13:52:26 [backends.py:430] Dynamo bytecode transform time: 18.31 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1227)\u001b[0;0m INFO 07-10 13:52:26 [backends.py:420] Using cache directory: /home/aiops/chenxw/.cache/vllm/torch_compile_cache/091db74a4d/rank_1_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1227)\u001b[0;0m INFO 07-10 13:52:26 [backends.py:430] Dynamo bytecode transform time: 18.32 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1226)\u001b[0;0m INFO 07-10 13:52:26 [backends.py:420] Using cache directory: /home/aiops/chenxw/.cache/vllm/torch_compile_cache/091db74a4d/rank_0_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1226)\u001b[0;0m INFO 07-10 13:52:26 [backends.py:430] Dynamo bytecode transform time: 18.52 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1229)\u001b[0;0m INFO 07-10 13:52:27 [backends.py:420] Using cache directory: /home/aiops/chenxw/.cache/vllm/torch_compile_cache/091db74a4d/rank_3_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1229)\u001b[0;0m INFO 07-10 13:52:27 [backends.py:430] Dynamo bytecode transform time: 18.97 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1229)\u001b[0;0m INFO 07-10 13:52:45 [backends.py:136] Cache the graph of shape None for later use\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1228)\u001b[0;0m INFO 07-10 13:52:45 [backends.py:136] Cache the graph of shape None for later use\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1226)\u001b[0;0m INFO 07-10 13:52:45 [backends.py:136] Cache the graph of shape None for later use\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1227)\u001b[0;0m INFO 07-10 13:52:45 [backends.py:136] Cache the graph of shape None for later use\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1228)\u001b[0;0m INFO 07-10 13:53:35 [backends.py:148] Compiling a graph for general shape takes 67.95 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1229)\u001b[0;0m INFO 07-10 13:53:35 [backends.py:148] Compiling a graph for general shape takes 67.38 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1226)\u001b[0;0m INFO 07-10 13:53:35 [backends.py:148] Compiling a graph for general shape takes 68.58 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1227)\u001b[0;0m INFO 07-10 13:53:36 [backends.py:148] Compiling a graph for general shape takes 69.36 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1227)\u001b[0;0m INFO 07-10 13:54:13 [monitor.py:33] torch.compile takes 87.68 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1226)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=1229)\u001b[0;0m INFO 07-10 13:54:13 [monitor.py:33] torch.compile takes 87.11 s in total\n",
      "INFO 07-10 13:54:13 [monitor.py:33] torch.compile takes 86.35 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1228)\u001b[0;0m INFO 07-10 13:54:13 [monitor.py:33] torch.compile takes 86.26 s in total\n",
      "INFO 07-10 13:54:14 [kv_cache_utils.py:634] GPU KV cache size: 815,440 tokens\n",
      "INFO 07-10 13:54:14 [kv_cache_utils.py:637] Maximum concurrency for 20,000 tokens per request: 40.77x\n",
      "INFO 07-10 13:54:14 [kv_cache_utils.py:634] GPU KV cache size: 807,248 tokens\n",
      "INFO 07-10 13:54:14 [kv_cache_utils.py:637] Maximum concurrency for 20,000 tokens per request: 40.36x\n",
      "INFO 07-10 13:54:14 [kv_cache_utils.py:634] GPU KV cache size: 807,248 tokens\n",
      "INFO 07-10 13:54:14 [kv_cache_utils.py:637] Maximum concurrency for 20,000 tokens per request: 40.36x\n",
      "INFO 07-10 13:54:14 [kv_cache_utils.py:634] GPU KV cache size: 815,440 tokens\n",
      "INFO 07-10 13:54:14 [kv_cache_utils.py:637] Maximum concurrency for 20,000 tokens per request: 40.77x\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1226)\u001b[0;0m INFO 07-10 13:54:43 [custom_all_reduce.py:195] Registering 4891 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1227)\u001b[0;0m INFO 07-10 13:54:44 [custom_all_reduce.py:195] Registering 4891 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1228)\u001b[0;0m INFO 07-10 13:54:45 [custom_all_reduce.py:195] Registering 4891 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1229)\u001b[0;0m INFO 07-10 13:54:45 [custom_all_reduce.py:195] Registering 4891 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=1229)\u001b[0;0m INFO 07-10 13:54:45 [gpu_model_runner.py:1686] Graph capturing finished in 31 secs, took 0.58 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=1228)\u001b[0;0m INFO 07-10 13:54:45 [gpu_model_runner.py:1686] Graph capturing finished in 31 secs, took 0.58 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1226)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=1227)\u001b[0;0m INFO 07-10 13:54:45 [gpu_model_runner.py:1686] Graph capturing finished in 31 secs, took 0.58 GiB\n",
      "INFO 07-10 13:54:45 [gpu_model_runner.py:1686] Graph capturing finished in 31 secs, took 0.58 GiB\n",
      "INFO 07-10 13:54:46 [core.py:159] init engine (profile, create kv cache, warmup model) took 157.67 seconds\n",
      "INFO 07-10 13:54:46 [core_client.py:439] Core engine process 0 ready.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(\n",
    "    model=pretrained_model,\n",
    "    dtype=\"bfloat16\",\n",
    "    tensor_parallel_size=4,\n",
    "    gpu_memory_utilization=0.85,\n",
    "    max_model_len=20000,\n",
    "    seed=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfcf3fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    temperature=0.8,\n",
    "    top_p=1.0,\n",
    "    top_k=-1,\n",
    "    min_p=0.0,\n",
    "    max_tokens=10000,\n",
    "    stop=[\"</answer>\", \"User:\", \"Human:\", \"Assistant:\", \"<|im_end|>\", \"<|endoftext|>\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0536027",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_prompt=\"\"\"<|im_start|>system\\nYou are a helpful assistant help user solve problems. <|im_end|>\\n<|im_start|>user\\nYou need to think first then write python script. You should use input() to input and print() to output in your script. \\nThis is the problem:\\nBamboo Blossoms\\n\\nThe bamboos live for decades, and at the end of their lives, they flower to make their seeds. Dr. ACM, a biologist, was fascinated by the bamboos in blossom in his travel to Tsukuba. He liked the flower so much that he was tempted to make a garden where the bamboos bloom annually. Dr. ACM started research of improving breed of the bamboos, and finally, he established a method to develop bamboo breeds with controlled lifetimes. With this method, he can develop bamboo breeds that flower after arbitrarily specified years.\\n\\nLet us call bamboos that flower k years after sowing \\\"k-year-bamboos.\\\" k years after being sowed, k-year-bamboos make their seeds and then die, hence their next generation flowers after another k years. In this way, if he sows seeds of k-year-bamboos, he can see bamboo blossoms every k years. For example, assuming that he sows seeds of 15-year-bamboos, he can see bamboo blossoms every 15 years; 15 years, 30 years, 45 years, and so on, after sowing.\\n\\nDr. ACM asked you for designing his garden. His garden is partitioned into blocks, in each of which only a single breed of bamboo can grow. Dr. ACM requested you to decide which breeds of bamboos should he sow in the blocks in order to see bamboo blossoms in at least one block for as many years as possible.\\n\\nYou immediately suggested to sow seeds of one-year-bamboos in all blocks. Dr. ACM, however, said that it was difficult to develop a bamboo breed with short lifetime, and would like a plan using only those breeds with long lifetimes. He also said that, although he could wait for some years until he would see the first bloom, he would like to see it in every following year. Then, you suggested a plan to sow seeds of 10-year-bamboos, for example, in different blocks each year, that is, to sow in a block this year and in another block next year, and so on, for 10 years. Following this plan, he could see bamboo blossoms in one block every year except for the first 10 years. Dr. ACM objected again saying he had determined to sow in all blocks this year.\\n\\nAfter all, you made up your mind to make a sowing plan where the bamboos bloom in at least one block for as many consecutive years as possible after the first m years (including this year) under the following conditions:\\n\\n* the plan should use only those bamboo breeds whose lifetimes are m years or longer, and\\n* Dr. ACM should sow the seeds in all the blocks only this year.\\n\\n\\nInput\\n\\nThe input consists of at most 50 datasets, each in the following format.\\n\\nm n\\n\\n\\nAn integer m (2 ≤ m ≤ 100) represents the lifetime (in years) of the bamboos with the shortest lifetime that Dr. ACM can use for gardening. An integer n (1 ≤ n ≤ 500,000) represents the number of blocks.\\n\\nThe end of the input is indicated by a line containing two zeros.\\n\\nOutput\\n\\nNo matter how good your plan is, a \\\"dull-year\\\" would eventually come, in which the bamboos do not flower in any block. For each dataset, output in a line an integer meaning how many years from now the first dull-year comes after the first m years.\\n\\nNote that the input of m = 2 and n = 500,000 (the last dataset of the Sample Input) gives the largest answer.\\n\\nSample Input\\n\\n\\n3 1\\n3 4\\n10 20\\n100 50\\n2 500000\\n0 0\\n\\n\\nOutput for the Sample Input\\n\\n\\n4\\n11\\n47\\n150\\n7368791\\n\\n\\n\\n\\n\\n\\nExample\\n\\nInput\\n\\n3 1\\n3 4\\n10 20\\n100 50\\n2 500000\\n0 0\\n\\n\\nOutput\\n\\n4\\n11\\n47\\n150\\n7368791\\n\\n<|im_end|>\\n<|im_start|>assistant\\n\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "289f1a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bae8523004b49c5a598bf1f608bf47a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outputs = llm.generate([code_prompt]*16, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c249f880",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5847740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def extract_code(full_output):\n",
    "    matches = re.findall(r\"```python(.*?)```\", full_output, re.DOTALL)\n",
    "    if matches:\n",
    "        code_output = matches[-1].strip()\n",
    "    else:\n",
    "        code_output = \"We can not extract the code in the output. \"\n",
    "    return code_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b97edc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======code=====\n",
      "We can not extract the code in the output. \n",
      "======code=====\n",
      "We can not extract the code in the output. \n",
      "======code=====\n",
      "We can not extract the code in the output. \n",
      "======code=====\n",
      "We can not extract the code in the output. \n",
      "======code=====\n",
      "We can not extract the code in the output. \n",
      "======code=====\n",
      "We can not extract the code in the output. \n",
      "======code=====\n",
      "We can not extract the code in the output. \n",
      "======code=====\n",
      "We can not extract the code in the output. \n",
      "======code=====\n",
      "We can not extract the code in the output. \n",
      "======code=====\n",
      "We can not extract the code in the output. \n",
      "======code=====\n",
      "We can not extract the code in the output. \n",
      "======code=====\n",
      "We can not extract the code in the output. \n",
      "======code=====\n",
      "We can not extract the code in the output. \n",
      "======code=====\n",
      "We can not extract the code in the output. \n",
      "======code=====\n",
      "We can not extract the code in the output. \n",
      "======code=====\n",
      "We can not extract the code in the output. \n"
     ]
    }
   ],
   "source": [
    "for o in outputs:\n",
    "    out = o.outputs[0].text\n",
    "    print(\"======code=====\")\n",
    "    print(extract_code(out))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6275e02e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "verl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
